# Appearance Codes using Joint Embedding Learning of Multiple Modalities
## Evan Dogariu & Alex Zhang

Hi! To whom it may concern, we detail below the organization of this repository, as well as where the entrypoints are in order to train our framework and reproduce our experiments. Furthermore, we hope that the structure of this code is clean and general enough to perhaps be used elsewhere by anyone who is training ensemble models (_we are particularly proud of `trainer.py` and its use of the abstract class defined in `model.py` and instantiated in `ensemble.py` :)_ ). 


### Entrypoints
The main entrypoints are the following:
- `run.py`: This is the python script to run that will train our ensemble model of 3 encoders and 1 decoder, with or without the contrastive losses. Hyperparameters such as batch size, which dataset to use, model architecture, and loss weights are included in the top of this file, and the selections we used for our experiments are included in the paper (`paper.pdf`). If you intend to train our setup with your own dataset, see below for how to do so.
- `inference.ipynb`: This is the notebook with which to inference through the model and generate the figures in our paper. The structure of this notebook should be straightforward, but for clarity the top of the notebook initializes the models, the middle inferences through a single datapoint in the dataset, and the bottom makes tSNE plots of the embedding spaces.
- `alex/appearance_codes.py`: For the implementation of the baseline method of directly optimizing appearance codes (the prior approach that proposed by [this paper](https://arxiv.org/pdf/1707.05776.pdf) and used in BlockNeRF, NeRF-in-the-wild), the entrypoint is in this file, which is located in the "alex" branch of this repo.

### Some Example Results
![rgbd_contrastive_structure_view](https://github.com/edogariu/alex-zhang/assets/78564140/57156728-82f5-4d08-a1f2-99a69ffdf2c1)
*Example tSNE of structure latent space for the RGB-D dataset generated by our method with contrastive losses enabled. Color indicates view direction. Note the clustering with respect to view direction, a structural feature.*


![](https://github.com/edogariu/alex-zhang/assets/78564140/eb134202-e531-4fb3-a725-ecd4b1dd4140")
*Examples of modifying night-time datapoints from the RADIATE dataset using the mean day-time embedding, generated with `inference.ipynb` on a model trained with contrastive losses enabled.*




### The Rest
The remaining files are detailed below:
- `datasets.py`: Here, we have the code to construct dataloaders for the two datasets used (either the [RGB-D Objects Dataset](https://rgbd-dataset.cs.washington.edu/#:~:text=The%20RGB%2DD%20Object%20Dataset%20is%20a%20large%20dataset%20of,relationships%20(similar%20to%20ImageNet).) or the [RADIATE Dataset](https://pro.hw.ac.uk/radiate/)), including all required resizes, reprojections, and normalizations.
- `ensemble.py`: Here, we subclass the abstract class in `model.py` to implement forward passes, training backward passes, and evaluation metrics on our ensemble as a whole. This keeps the code clean and allows for flexibility and a single place in the code to touch.
- `losses.py`: This is where the losses are implemented (minus the KL-divergence loss, which is implmented directly in `vae.py` as it is VAE-specific). 
- `model.py`: This contains an abstract class that makes working with an ensemble of models seamless and headache free. It handles training steps, different optimizers for different models, checkpointing, etc..
- `radiate.py`: This contains a handful of tools stolen shamelessly from the [RADIATE SDK](https://github.com/marcelsheeny/radiate_sdk) in order to interface with the raw RADIATE data and perform reprojection, etc.. **credit to the authors!**
- `requirements.txt`: Requirements for the repo
- `trainer.py`: A training script that implements checkpointing and an evaluation patience algorithm (which I love and will always always use from now on). This is designed to work seamlessly with any implementation of an ensemble that subclasses `ModelBase`.
- `utils.py`: Generic DL utilities we needed.
- `vae.py`: Implementations of the encoder and decoder for a VAE, which are used as the components of the ensemble.

### Custom Encoder or Decoder Architecture
Let's say you wanted to make use of a different encoder or decoder architecture (such as using a PointNet to directly encode 3D LiDAR pointclouds, or using a diffusion or GAN model to perform RGB generation from the latent representations). Then, you would simply modify `ensemble.py` and `run.py` to suit your liking, passing different encoder and decoder architectures of your design into the constructor of the `Ensemble` class and modifying the `infer()`, `loss()`, `eval_err()`, and potentially `_split_data()` methods. If you intend to use a different type of data as well (such as unreprojected, raw 3D LiDAR pointclouds, you must make the necessary modifications in `datasets.py` as well).

### Custom Dataset
To apply a custom dataset, simply replicate our design of the two datasets in `datasets.py`. In particular, you must make a dataset class with a `get_dataloader()` method to return PyTorch dataloaders for the various splits. The rest is up to you :)

<!-- # alex-zhang

Hi alex. 

I coded up a whole bunch of stuff. Everything should actually be clean and polished, and the entrypoint is `run.py`. The hyperparameters of interest are there. I've tested it and it seems to work to train a VAE and do the right thing: take a look at the below example (original on left & reconstruction on right), which was trained on my mac in liek 10 minutes lol (its blurry, which might have something to do with me not turning on the KL divergence loss):

![image](https://user-images.githubusercontent.com/78564140/236430176-4a02ef47-0cd6-4202-9483-28380fd4a4b1.png)
![image](https://user-images.githubusercontent.com/78564140/236432497-d0af29a9-1e1c-4610-8f21-2f1718e09577.png)

### things i need to fix and/or next steps
- for some reason, whenever i set any of the loss weights (kl, contrastive, and anticontrastive) to something nonzero, loss eventually goes to nan. I think im calculating something wrong or dividing by something that goes to 0, idk ill fix it tomorrow
- gotta get the nuscenes dataset goin as well
- GOTTA ANALYZE DA LATENT SPACE. its kinda useless since i dont have the contrastive learning enabled (it's set to 0 in `run.py` and when i increase it the loss goes to nan, i must fix this), but still may be interesting. since the dataset im playing with only has like 14 scenes, it seems reasonable to expect good clustering

### things u need to do to set it up (i might write a batch script for this tmrw morning idc)
- `pip install requirements.txt` or whatever. im using a venv but u just gotta get those libraries somehow
- make the following folders inside the main directory
  - make a folder called `checkpoints/` and put two folders inside of it; one called `models/` and one called `optimizers/`
  - make a folder called `plots/`
  - make a folder called `data/`; inside of it, place the unzipped version of this file http://rgbd-dataset.cs.washington.edu/dataset/rgbd-scenes-v2/rgbd-scenes-v2_imgs.zip, gotten from this link http://rgbd-dataset.cs.washington.edu/dataset/rgbd-scenes-v2/
    - this is a RGB-D dataset of some random objects and scenes. i wanna make sure training and everything is stable on this first before doing stuff with more exotic datasets and especially before LIDAR
    - in `datasets.py` u can uncomment the thing to actually read through the dataset. it takes a while to load all the files, so i loaded them once and stored em in a `.npy` file to keep loading from. i recommend this :)
    - the images are in fairly high res. i downsampled to 64x64 for my lil experiment on the mac, but i think the full res is 640x480 or something
  


#### also if u want code to inference a thingy to reconstruct, here it is

    import matplotlib.pyplot as plt

    import torch

    from vae import Encoder, Decoder
    from datasets import RGBDDataset
    from ensemble import Ensemble
    from utils import DEVICE

    resolution = (64, 64)

    model_args = {'appearance_latent_dim': 64,
                  'structure_latent_dim': 64,
                  'hidden_dims': [32, 64, 128, 256]}

    dataset = RGBDDataset(resolution)
    dataloader = dataset.get_dataloader('all', 1)

    # make ensemble
    models = {'RGB_appearance': Encoder(input_shape=[3, *resolution], latent_dim=model_args['appearance_latent_dim'], hidden_dims=model_args['hidden_dims']), 
              'RGB_structure': Encoder(input_shape=[3, *resolution], latent_dim=model_args['structure_latent_dim'], hidden_dims=model_args['hidden_dims']), 
              'DEPTH_structure': Encoder(input_shape=[1, *resolution], latent_dim=model_args['structure_latent_dim'], hidden_dims=model_args['hidden_dims'])}
    models['RGB_decoder'] = Decoder(input_shape=[3, *resolution], 
                                    latent_dim=model_args['appearance_latent_dim'] + model_args['structure_latent_dim'], 
                                    encoder_conv_out_shape=models['RGB_appearance'].conv_out_shape, 
                                    hidden_dims=model_args['hidden_dims'])
    model = Ensemble(models, loss_weights=None).float().to(DEVICE)
    model.load_checkpoint()

    with torch.no_grad():
        for x, in dataloader:
            out = model.infer(x)
            emb = torch.cat((out.RGB_appearance, out.RGB_structure), dim=-1)
            rec = models['RGB_decoder'](emb)

            x = (x.squeeze()[:3].permute(1, 2, 0).cpu().numpy() + 1) / 2
            rec = (rec.squeeze().permute(1, 2, 0).cpu().numpy() + 1) / 2

            fig, ax = plt.subplots(1, 2)
            ax[0].imshow(x)
            ax[1].imshow(rec)

            break -->
